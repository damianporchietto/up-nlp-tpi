# RAG Solution: Asistente Inteligente para Tr√°mites Gubernamentales

## üìú Resumen Ejecutivo

Este documento ofrece una explicaci√≥n completa y detallada de la soluci√≥n de **Generaci√≥n Aumentada por Recuperaci√≥n (RAG)** implementada para crear un asistente de IA especializado en los tr√°mites gubernamentales de la Provincia de C√≥rdoba, Argentina.

El sistema est√° dise√±ado para ser **robusto, configurable y acad√©micamente riguroso**, abordando los desaf√≠os comunes de los sistemas RAG y aplicando las mejores pr√°cticas de la industria.

**Capacidades Clave:**
- **Respuestas Basadas en Evidencia**: Contesta preguntas utilizando √∫nicamente la informaci√≥n oficial cargada en su base de conocimiento.
- **Configuraci√≥n Flexible**: Permite cambiar f√°cilmente los modelos de lenguaje (LLM) y de embedding (OpenAI, Ollama, HuggingFace).
- **Validaci√≥n Autom√°tica**: Previene errores cr√≠ticos al garantizar que los modelos utilizados para indexar y consultar sean consistentes.
- **Optimizaci√≥n Inteligente**: Ajusta autom√°ticamente sus par√°metros internos (como el *chunking*) seg√∫n el modelo seleccionado para maximizar la calidad.

---

## üßê 1. ¬øQu√© es un Sistema RAG?

Un sistema de **Generaci√≥n Aumentada por Recuperaci√≥n (RAG)** es una arquitectura de inteligencia artificial que combina lo mejor de dos mundos:

1.  **Modelos de Lenguaje Pre-entrenados (LLMs)**: Como GPT-4 o Llama3, que son excelentes para entender y generar texto, pero su conocimiento est√° "congelado" en el momento en que fueron entrenados y pueden "alucinar" o inventar informaci√≥n.
2.  **Bases de Conocimiento Externas**: Colecciones de documentos privados y actualizados (PDFs, JSONs, etc.) que contienen informaci√≥n espec√≠fica y ver√≠dica.

**El Proceso RAG en 3 Pasos:**

1.  **üîç Recuperar (Retrieve)**: Cuando un usuario hace una pregunta, el sistema no se la pasa directamente al LLM. Primero, busca en su base de conocimiento (en nuestro caso, un **Vector Store**) los fragmentos de texto m√°s relevantes para la pregunta.
2.  **‚ûï Aumentar (Augment)**: El sistema toma los fragmentos recuperados (el "contexto") y los inserta en un *prompt* junto con la pregunta original del usuario.
3.  **‚úçÔ∏è Generar (Generate)**: Finalmente, le entrega este *prompt aumentado* al LLM, d√°ndole la instrucci√≥n: "Responde a esta pregunta bas√°ndote *√∫nicamente* en este contexto".

Esto obliga al LLM a basar su respuesta en los documentos oficiales, reduciendo dr√°sticamente las alucinaciones y permiti√©ndole responder sobre datos que no estaban en su entrenamiento original.

---

## ‚öôÔ∏è 2. Arquitectura de Nuestra Soluci√≥n

Nuestro sistema sigue la arquitectura RAG cl√°sica, pero con optimizaciones y validaciones clave en cada etapa.

```mermaid
graph TD
    subgraph Fase 1: Ingesta de Datos (Offline)
        A["üìÑ Documentos JSON<br/>(en /docs)"] --> B["üîß Preprocessing<br/>(Optimizado para Transformers)"];
        B --> C["üì¶ Chunking Adaptativo<br/>(Fragmentaci√≥n Inteligente)"];
        C --> D["üß† Conversi√≥n a Vectores<br/>(Modelo de Embedding)"];
        D --> E["üíæ Vector Store (FAISS)<br/>(Guardado en /storage)"];
        E --> F["üìù Metadatos<br/>(index_metadata.json)"];
    end

    subgraph Fase 2: Ciclo de Consulta (Online)
        G["‚ùì Pregunta del Usuario"] --> H["üß† Conversi√≥n a Vector<br/>(Mismo Modelo de Embedding)"];
        H --> I{üîç B√∫squeda de Similitud};
        E --> I;
        I --> J["üìö Contexto Recuperado<br/>(Top-K Chunks Relevantes)"];
        G --> K{‚ûï Prompt Aumentado};
        J --> K;
        K --> L["ü§ñ LLM (Generaci√≥n)<br/>(gpt-4o-mini, llama3, etc.)"];
        L --> M["‚úÖ Respuesta Final + Fuentes"];
    end
```

### Componentes Clave:

#### **a. Ingesta y Preprocesamiento (`ingest.py`, `preprocessing.py`)**
El proceso comienza con los documentos JSON ubicados en la carpeta `rag_flask/docs/`.

-   **Preprocessing Transformer-Optimizado**: A diferencia de t√©cnicas de NLP m√°s antiguas, **no eliminamos stopwords (palabras comunes como "el", "de") ni acentos**. Los modelos Transformer modernos utilizan estas caracter√≠sticas para entender el contexto y la sem√°ntica. Nuestra limpieza se centra en normalizar espacios y eliminar caracteres problem√°ticos sin destruir la estructura del lenguaje.
-   **Script de Ingesta**: El script `ingest.py` orquesta todo el proceso de preparaci√≥n de datos, desde leer los archivos hasta construir la base de datos vectorial final.

#### **b. Chunking Adaptativo (Fragmentaci√≥n Inteligente)**
Los documentos largos no pueden ser procesados directamente por los LLMs. Por eso, los dividimos en fragmentos m√°s peque√±os o "chunks".

-   **Estrategia**: Usamos `RecursiveCharacterTextSplitter`, que intenta dividir el texto por p√°rrafos, saltos de l√≠nea y frases, preservando as√≠ la cohesi√≥n sem√°ntica.
-   **Optimizaci√≥n Autom√°tica**: El tama√±o de estos chunks no es fijo. El sistema lo ajusta autom√°ticamente (`get_optimal_chunk_config`) bas√°ndose en el modelo de embedding que se est√© utilizando. Esto es crucial, ya que cada modelo tiene una "ventana de contexto" √≥ptima.
-   **Overlap (Solapamiento)**: Cada chunk comparte un peque√±o porcentaje de contenido (10-15%) con el chunk anterior y siguiente. Esto evita que una idea importante se corte justo en la frontera entre dos fragmentos.

#### **c. Embeddings y Vector Store (`model_providers.py`, `storage/`)**
-   **Embeddings**: Son representaciones num√©ricas (vectores) del texto. Un modelo de embedding convierte cada chunk en un vector de forma que los chunks con significados similares tengan vectores cercanos en el espacio.
-   **Vector Store**: Utilizamos **FAISS** (de Facebook AI), una biblioteca ultra-eficiente para buscar vectores similares. Nuestra base de datos vectorial se guarda en la carpeta `rag_flask/storage/`.

#### **d. Cadena de Consulta (`rag_chain.py`)**
Esta es la l√≥gica central que se ejecuta cuando un usuario realiza una consulta.

-   **Recuperaci√≥n**: Se vectoriza la pregunta del usuario y se usa FAISS para encontrar los `k` chunks m√°s similares (por defecto, `k=4`).
-   **Aumentaci√≥n (Prompting)**: Se utiliza una plantilla de prompt (`PROMPT_TEMPLATE`) que instruye claramente al LLM sobre su rol y le obliga a usar el contexto proporcionado.
-   **Generaci√≥n**: El LLM sintetiza la informaci√≥n de los chunks recuperados en una respuesta coherente y bien redactada.

---

## üí° 3. Decisiones T√©cnicas Clave

Este sistema no es un RAG gen√©rico. Se han implementado varias mejoras cruciales para responder a problemas del mundo real.

#### **1. Configuraci√≥n Centralizada (`config/rag_config.json`)**
Para evitar tener par√°metros "hardcodeados" en el c√≥digo, toda la configuraci√≥n principal reside en `rag_flask/config/rag_config.json`. Esto permite modificar el comportamiento del sistema (cambiar modelos, prompts, etc.) sin tocar una sola l√≠nea de c√≥digo Python.

#### **2. Validaci√≥n de Consistencia de Modelos (¬°Cr√≠tico!)**
Un error catastr√≥fico en los sistemas RAG ocurre cuando se indexan los documentos con un modelo de embedding (ej. `text-embedding-3-large` de OpenAI) y luego se intenta hacer una consulta con otro (ej. `nomic-embed-text` de Ollama). Los vectores son incompatibles y los resultados de b√∫squeda son basura.

-   **Nuestra Soluci√≥n**: Durante la ingesta (`ingest.py`), guardamos un archivo de metadatos (`index_metadata.json`) que registra qu√© modelo se us√≥. Al iniciar la consulta (`rag_chain.py`), el sistema **valida** que el modelo actual coincida con el de los metadatos. Si no es as√≠, lanza un error claro y expl√≠cito, evitando fallos silenciosos.

---

## üöÄ 4. Gu√≠a de Uso R√°pido

Sigue estos pasos para poner en marcha el sistema.

#### **Paso 0: Configuraci√≥n**
Antes de nada, revisa y ajusta `rag_flask/config/rag_config.json`. Aqu√≠ puedes definir los modelos por defecto.

```json
{
  "llm_config": {
    "default_provider": "ollama",
    "default_model": "llama3"
  },
  "embedding_config": {
    "default_provider": "ollama",
    "default_model": "nomic-embed-text"
  },
  // ... resto de la configuraci√≥n
}
```

Si usas OpenAI, aseg√∫rate de tener tu clave de API en un archivo `.env`:
```
OPENAI_API_KEY="sk-..."
```

#### **Paso 1: Ingesta de Datos**
Este comando procesar√° los documentos de la carpeta `/docs`, los convertir√° en vectores usando el modelo de embedding de tu configuraci√≥n y crear√° el √≠ndice FAISS en `/storage`.

```bash
# Aseg√∫rate de estar en el directorio ra√≠z del proyecto
python rag_flask/ingest.py
```
*Este paso solo necesitas hacerlo una vez, o cada vez que actualices tus documentos.*

#### **Paso 2: Iniciar el Servidor API**
Esto levanta la aplicaci√≥n web con la que podr√°s interactuar.

```bash
python rag_flask/app.py
```
El servidor se iniciar√° en `http://localhost:5000`.

Puedes **sobrescribir** la configuraci√≥n por defecto al iniciar el servidor:
```bash
# Ejemplo para usar modelos de OpenAI en lugar de los del JSON
python rag_flask/app.py --llm-provider openai --embedding-provider openai
```

#### **Paso 3: Interactuar con el Sistema**

1.  **Interfaz Web (Recomendado)**:
    -   Abre tu navegador y ve a `http://localhost:5000`.
    -   Encontrar√°s una interfaz sencilla para escribir tus preguntas y ver las respuestas generadas.

2.  **API (para desarrolladores)**:
    Puedes usar `curl` o cualquier cliente de API para interactuar con los endpoints.

    -   **Realizar una consulta:**
        ```bash
        curl -X POST http://localhost:5000/ask \
             -H "Content-Type: application/json" \
             -d '{"message": "¬øQu√© necesito para obtener un certificado de antecedentes?"}'
        ```

    -   **Verificar la salud del sistema:**
        ```bash
        curl http://localhost:5000/health
        ```

    -   **Ver la configuraci√≥n actual:**
        ```bash
        curl http://localhost:5000/config
        ```

---

## üìÅ 5. Estructura del Proyecto

-   `rag_flask/`: Directorio principal de la aplicaci√≥n.
    -   `app.py`: Servidor web Flask y definici√≥n de endpoints.
    -   `rag_chain.py`: Contiene la l√≥gica principal del pipeline RAG.
    -   `ingest.py`: Script para procesar documentos y construir el √≠ndice vectorial.
    -   `preprocessing.py`: Funciones de limpieza y preparaci√≥n de texto.
    -   `model_providers.py`: Abstracci√≥n para comunicarse con diferentes APIs de modelos (OpenAI, Ollama, etc.).
    -   `config/rag_config.json`: **Archivo de configuraci√≥n central.**
    -   `docs/`: Aqu√≠ debes colocar los documentos fuente en formato JSON.
    -   `storage/`: Donde se guarda el √≠ndice FAISS y los metadatos de validaci√≥n.

---

## ‚úÖ 6. Conclusi√≥n

Esta soluci√≥n RAG representa un sistema completo, bien fundamentado y listo para producci√≥n. No solo implementa la arquitectura b√°sica, sino que incorpora **soluciones expl√≠citas a problemas cr√≠ticos** como la consistencia de modelos y la optimizaci√≥n de par√°metros. La externalizaci√≥n de la configuraci√≥n lo hace flexible y f√°cil de mantener, sentando las bases para futuras mejoras como la evaluaci√≥n continua y el soporte multimodal. 