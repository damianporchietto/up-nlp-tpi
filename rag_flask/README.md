# Asistente RAG de TrÃ¡mites de CÃ³rdoba - VersiÃ³n 2.0

Servicio de **GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG)** para consultas sobre trÃ¡mites y servicios gubernamentales de la Provincia de CÃ³rdoba. 

## ðŸš€ **Mejoras Implementadas (v2.0)**

### âœ… **Correcciones CrÃ­ticas Basadas en Observaciones AcadÃ©micas**

1. **ðŸ”§ Preprocesamiento Optimizado para Transformers**
   - âŒ **ELIMINADO**: RemociÃ³n de stopwords (contraproducente para modelos modernos)
   - âŒ **ELIMINADO**: RemociÃ³n de tildes y acentos (importante para espaÃ±ol)
   - âœ… **MANTENIDO**: Solo normalizaciÃ³n bÃ¡sica apropiada para Transformers

2. **ðŸ“ Chunking Inteligente y Optimizado**
   - âœ… **TamaÃ±os adaptativos** basados en modelo de embedding (256-512 tokens)
   - âœ… **Overlap optimizado** (10% del chunk size)
   - âœ… **JustificaciÃ³n tÃ©cnica** de parÃ¡metros basada en investigaciÃ³n RAG 2024
   - âœ… **ConsideraciÃ³n de ventana de contexto** de modelos

3. **ðŸ”’ ValidaciÃ³n de Consistencia de Modelos**
   - âœ… **Metadatos automÃ¡ticos** guardados durante indexaciÃ³n
   - âœ… **ValidaciÃ³n obligatoria** entre modelo de indexaciÃ³n y consulta
   - âœ… **DetecciÃ³n de inconsistencias** con mensajes de error claros

4. **ðŸ“Š EspecificaciÃ³n Clara de Modelos**
   - âœ… **DocumentaciÃ³n explÃ­cita** de quÃ© modelo se usa para quÃ©
   - âœ… **Endpoints informativos** con detalles del sistema
   - âœ… **Arquitectura claramente definida**: Embedding vs LLM

5. **ðŸŽ¯ Framework de EvaluaciÃ³n Robusto**
   - âœ… **MÃ©tricas cuantitativas** (Recall@K, Precision@K, MRR, similitud semÃ¡ntica)
   - âœ… **Dataset de evaluaciÃ³n expandible**
   - âœ… **Benchmarking automÃ¡tico** de configuraciones
   - âœ… **Reportes con anÃ¡lisis** y recomendaciones

## ðŸ“ **Estructura del Proyecto**

```
rag_flask/
â”œâ”€â”€ app.py                    # ðŸŒ API Flask con validaciÃ³n de modelos
â”œâ”€â”€ rag_chain.py              # ðŸ§  Pipeline RAG con validaciÃ³n automÃ¡tica
â”œâ”€â”€ ingest.py                 # ðŸ“¥ Ingesta con chunking optimizado
â”œâ”€â”€ preprocessing.py          # ðŸ”§ Preprocesamiento para Transformers
â”œâ”€â”€ model_providers.py        # ðŸ¤– GestiÃ³n de proveedores de modelos
â”œâ”€â”€ evaluation.py             # ðŸ“Š Framework de evaluaciÃ³n comprehensivo
â”œâ”€â”€ requirements.txt          # ðŸ“¦ Dependencias
â”œâ”€â”€ docs/                     # ðŸ“„ Documentos fuente (JSON por ministerio)
â”œâ”€â”€ storage/                  # ðŸ’¾ Ãndice FAISS + metadatos
â”‚   â””â”€â”€ index_metadata.json   # ðŸ” ValidaciÃ³n de consistencia
â””â”€â”€ test/                     # ðŸ§ª Tests y evaluaciones
```

## ðŸ—ï¸ **Arquitectura de Modelos - EspecificaciÃ³n TÃ©cnica**

### **Modelo de Embedding** (BÃºsqueda de Similaridad)
- **PropÃ³sito**: Convertir texto a vectores para bÃºsqueda en Ã­ndice
- **Uso**: IndexaciÃ³n de documentos + consultas de usuario
- **Requisito CRÃTICO**: Debe ser el mismo modelo para ambos procesos
- **ConfiguraciÃ³n**: `--embedding-provider` y `--embedding-model`

### **Modelo LLM** (GeneraciÃ³n de Respuestas)
- **PropÃ³sito**: Generar respuesta final basada en contexto recuperado
- **Uso**: Solo para generaciÃ³n de texto
- **Independencia**: Puede ser diferente del modelo de embedding
- **ConfiguraciÃ³n**: `--llm-provider` y `--llm-model`

## ðŸš€ **Inicio RÃ¡pido**

### 1. ConfiguraciÃ³n del Entorno

```bash
# Crear entorno virtual
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cp .env.example .env
echo 'OPENAI_API_KEY=sk-...' >> .env
```

### 2. Crear Ãndice Vectorial (IndexaciÃ³n)

```bash
# Con OpenAI (recomendado para producciÃ³n)
python ingest.py --provider openai --model text-embedding-3-large

# Con Ollama (para desarrollo local)
python ingest.py --provider ollama --model nomic-embed-text

# El chunking se optimiza automÃ¡ticamente segÃºn el modelo
```

### 3. Ejecutar API (Consultas)

```bash
# IMPORTANTE: Usar los mismos modelos que en indexaciÃ³n
python app.py \
  --embedding-provider openai \
  --embedding-model text-embedding-3-large \
  --llm-provider openai \
  --llm-model gpt-4o-mini
```

## ðŸ” **ValidaciÃ³n AutomÃ¡tica de Modelos**

El sistema ahora **valida automÃ¡ticamente** que uses el mismo modelo de embedding:

```bash
# âœ… Correcto: Modelos consistentes
python ingest.py --provider openai --model text-embedding-3-large
python app.py --embedding-provider openai --embedding-model text-embedding-3-large

# âŒ Error detectado automÃ¡ticamente
python ingest.py --provider openai --model text-embedding-3-large  
python app.py --embedding-provider ollama --embedding-model nomic-embed-text
# â†’ ModelValidationError: Inconsistencia detectada
```

## ðŸ“Š **EvaluaciÃ³n y Benchmarking**

### EvaluaciÃ³n Simple

```bash
# Evaluar configuraciÃ³n actual
python evaluation.py --single-eval

# Resultados incluyen:
# - Score general (0-1)
# - Recall@3, Precision@3, MRR
# - Similitud semÃ¡ntica promedio
# - Tiempo de respuesta promedio
```

### Benchmark de Configuraciones

```bash
# Crear archivo de configuraciones a comparar
cat > benchmark_configs.json << EOF
[
  {
    "llm_provider": "openai",
    "llm_model": "gpt-4o-mini",
    "embedding_provider": "openai", 
    "embedding_model": "text-embedding-3-large"
  },
  {
    "llm_provider": "ollama",
    "llm_model": "mistral",
    "embedding_provider": "ollama",
    "embedding_model": "nomic-embed-text"
  }
]
EOF

# Ejecutar benchmark
python evaluation.py --config-file benchmark_configs.json
```

## ðŸŒ **Endpoints de la API**

### InformaciÃ³n del Sistema
```bash
# DocumentaciÃ³n completa con arquitectura
curl http://localhost:5000/

# Estado con validaciÃ³n de modelos
curl http://localhost:5000/health

# ConfiguraciÃ³n detallada del sistema
curl http://localhost:5000/config

# InformaciÃ³n tÃ©cnica para debugging
curl http://localhost:5000/system-info
```

### Consultas
```bash
curl -X POST http://localhost:5000/ask \
     -H 'Content-Type: application/json' \
     -d '{"message": "Â¿QuÃ© necesito para obtener un certificado de antecedentes?"}'
```

**Respuesta incluye**:
- `answer`: Respuesta generada
- `sources`: Documentos fuente con snippets
- `system_info`: Modelos usados y mÃ©tricas del sistema

## âš™ï¸ **ConfiguraciÃ³n de Chunking Optimizada**

El sistema ahora **ajusta automÃ¡ticamente** el chunking segÃºn el modelo:

| Modelo de Embedding | Chunk Size Ã“ptimo | JustificaciÃ³n |
|--------------------|--------------------|---------------|
| `text-embedding-3-large` | 512 tokens (~2300 chars) | Ventana de contexto optimizada |
| `nomic-embed-text` | 384 tokens (~1700 chars) | Modelo local balanceado |
| `all-MiniLM-L6-v2` | 256 tokens (~1150 chars) | Modelo liviano eficiente |

**Overlap**: 10% del chunk size (balance entre contexto y eficiencia)

## ðŸ§ª **Estrategia de Preprocesamiento Actualizada**

### âŒ **LO QUE NO HACEMOS (Contraproducente para Transformers)**
- Remover stopwords ("el", "la", "de", etc.)
- Quitar tildes y acentos 
- LemmatizaciÃ³n agresiva

### âœ… **LO QUE SÃ HACEMOS (Apropiado para Transformers)**
- NormalizaciÃ³n de espacios mÃºltiples
- ConversiÃ³n a minÃºsculas para consistencia
- EliminaciÃ³n solo de caracteres verdaderamente problemÃ¡ticos
- PreservaciÃ³n del contexto sintÃ¡ctico completo

## ðŸ”§ **Proveedores de Modelos Soportados**

### OpenAI (Recomendado para ProducciÃ³n)
```bash
# Embeddings
--embedding-provider openai --embedding-model text-embedding-3-large

# LLM
--llm-provider openai --llm-model gpt-4o-mini
```

### Ollama (Desarrollo Local)
```bash
# Instalar Ollama primero: https://ollama.ai
ollama pull nomic-embed-text
ollama pull mistral

# Usar en el sistema
--embedding-provider ollama --embedding-model nomic-embed-text
--llm-provider ollama --llm-model mistral
```

### HuggingFace (Open Source)
```bash
# Requiere instalaciÃ³n adicional
pip install transformers torch sentence-transformers

# ConfiguraciÃ³n
--embedding-provider huggingface --embedding-model BAAI/bge-large-en-v1.5
--llm-provider huggingface --llm-model google/flan-t5-xxl
```

## ðŸ“ˆ **Monitoreo y Debugging**

### Logs del Sistema
El sistema ahora proporciona logs detallados:
```
ðŸ”§ Inicializando RAG Pipeline:
   â€¢ LLM (generaciÃ³n): openai:gpt-4o-mini
   â€¢ Embeddings (bÃºsqueda): openai:text-embedding-3-large
ðŸ“ ConfiguraciÃ³n de chunking optimizada para openai:text-embedding-3-large
   â€¢ TamaÃ±o objetivo: 512 tokens (~2304 caracteres)
   â€¢ Overlap: 51 tokens (~230 caracteres, 10.0%)
âœ… ValidaciÃ³n de modelo exitosa
ðŸš€ Pipeline RAG inicializado correctamente
```

### MÃ©tricas de Performance
- **Tiempo de respuesta promedio**: Incluido en `/system-info`
- **Chunks recuperados por consulta**: Monitoreado automÃ¡ticamente
- **Diversidad de fuentes**: MÃ©trica de calidad del retrieval

## ðŸš¨ **SoluciÃ³n de Problemas Comunes**

### Error: Inconsistencia de Modelos
```
âŒ Inconsistencia de modelo de embedding:
   â€¢ Ãndice creado con: openai:text-embedding-3-large  
   â€¢ Consulta usando: ollama:nomic-embed-text
```
**SoluciÃ³n**: Usar el mismo modelo o recrear Ã­ndice
```bash
python ingest.py --provider ollama --model nomic-embed-text
```

### Performance Lenta
1. **Verificar modelo**: Modelos locales pueden ser mÃ¡s lentos
2. **Revisar chunking**: Chunks muy grandes afectan performance
3. **Optimizar retrieval**: Reducir `k` en configuraciÃ³n del retriever

### Calidad de Respuestas Baja
1. **Evaluar sistema**: `python evaluation.py --single-eval`
2. **Revisar chunking**: Evaluar si preserva contexto semÃ¡ntico
3. **Probar modelos**: Benchmark diferentes configuraciones

## ðŸ“š **Dataset de EvaluaciÃ³n**

El sistema incluye un dataset de evaluaciÃ³n expandible (`evaluation_dataset.json`):

```json
[
  {
    "query": "Â¿QuÃ© necesito para obtener un certificado de antecedentes?",
    "expected_answer": "Necesitas DNI en perfecto estado...",
    "relevant_doc_ids": ["certificado_antecedentes"],
    "category": "certificados",
    "difficulty": "easy"
  }
]
```

**Para ampliar**: Agrega mÃ¡s consultas con respuestas esperadas y documentos relevantes.

## ðŸ”¬ **InvestigaciÃ³n y FundamentaciÃ³n TÃ©cnica**

### Chunking Basado en InvestigaciÃ³n
- **Referencia**: "Evaluating the ideal chunk size for a rag system" (Vectorize, 2024)
- **Hallazgo clave**: 256-512 tokens Ã³ptimo para embeddings modernos
- **ImplementaciÃ³n**: ConfiguraciÃ³n adaptativa por modelo

### Preprocesamiento para Transformers
- **Principio**: Modelos entrenados con texto natural completo
- **Evidence**: Stopwords aportan contexto sintÃ¡ctico valioso
- **Resultado**: Mayor precisiÃ³n semÃ¡ntica en embeddings

## ðŸ¤ **ContribuciÃ³n y Desarrollo**

### Agregar Nuevos Documentos
```bash
# 1. Agregar archivos JSON a docs/MINISTERIO/
# 2. Recrear Ã­ndice
python ingest.py

# El sistema detecta automÃ¡ticamente nuevos archivos
```

### Desarrollo de Features
1. **Fork el repositorio**
2. **Crear branch**: `git checkout -b feature/nueva-funcionalidad`
3. **Ejecutar tests**: `python evaluation.py --single-eval`
4. **Commit y push**: Incluir resultados de evaluaciÃ³n
5. **Pull Request**: Con mÃ©tricas de performance

---

## ðŸ“‹ **Resumen de Correcciones Implementadas**

| ObservaciÃ³n AcadÃ©mica | âœ… CorrecciÃ³n Implementada |
|----------------------|---------------------------|
| Preprocesamiento contraproducente | Eliminado stopwords/tildes, optimizado para Transformers |
| Chunking sin justificaciÃ³n | ConfiguraciÃ³n adaptativa basada en investigaciÃ³n 2024 |
| Falta validaciÃ³n de modelos | Metadatos automÃ¡ticos + validaciÃ³n obligatoria |
| EspecificaciÃ³n poco clara | DocumentaciÃ³n exhaustiva de arquitectura |
| EvaluaciÃ³n no robusta | Framework con mÃ©tricas cuantitativas + benchmarking |
| Inconsistencias documentaciÃ³n | AlineaciÃ³n completa cÃ³digo-docs-funcionalidad |

**Resultado**: Sistema RAG robusto, bien documentado y tÃ©cnicamente fundamentado, listo para evaluaciÃ³n acadÃ©mica y uso en producciÃ³n.