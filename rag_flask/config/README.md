# ‚öôÔ∏è Gu√≠a de Configuraci√≥n de Modelos (`rag_config.json`)

Este documento explica c√≥mo configurar los modelos de Lenguaje (LLM) y de Embedding en el archivo `rag_config.json`. La flexibilidad de este sistema te permite combinar diferentes proveedores seg√∫n tus necesidades de rendimiento, costo y privacidad.

---

## üß† 1. Configuraci√≥n del LLM (`llm_config`)

El LLM es el "cerebro" que **genera las respuestas**. Se utiliza √∫nicamente en la fase final del proceso para sintetizar la informaci√≥n recuperada.

-   `"default_provider"`: Define qu√© servicio se usar√° para el LLM.
-   `"default_model"`: Especifica el modelo exacto a utilizar.
-   `"temperature"`: Controla la "creatividad" de la respuesta. Para un asistente basado en hechos, `0` es ideal.

### Opciones de Proveedores:

#### a) `openai`
-   **Descripci√≥n**: Modelos de alto rendimiento con costo por uso. Requiere una `OPENAI_API_KEY` en el archivo `.env`.
-   **Ejemplos de Modelos (`default_model`)**:
    -   `"gpt-4o-mini"`: **(Recomendado)** Excelente balance entre costo, velocidad y calidad.
    -   `"gpt-4-turbo"`: Modelo de m√°xima calidad, m√°s costoso.
    -   `"gpt-3.5-turbo"`: Opci√≥n m√°s econ√≥mica y r√°pida.

#### b) `ollama`
-   **Descripci√≥n**: Permite ejecutar modelos de c√≥digo abierto **localmente** en tu propia m√°quina. Ideal para privacidad y experimentaci√≥n sin costo. Debes tener [Ollama](https://ollama.com/) instalado y los modelos descargados (`ollama run llama3`).
-   **Ejemplos de Modelos (`default_model`)**:
    -   `"llama3"`: Modelo de √∫ltima generaci√≥n de Meta. Muy capaz.
    -   `"mistral"`: Excelente modelo de Mistral AI.
    -   `"gemma"`: Familia de modelos abiertos de Google.

#### c) `huggingface`
-   **Descripci√≥n**: Permite cargar modelos desde el Hugging Face Hub. M√°s complejo de configurar, generalmente requiere dependencias adicionales y m√°s recursos computacionales.
-   **Ejemplos de Modelos (`default_model`)**:
    -   `"google/flan-t5-large"`
    -   `"HuggingFaceH4/zephyr-7b-beta"`

---

## üîç 2. Configuraci√≥n de Embeddings (`embedding_config`)

El modelo de embedding es responsable de **entender el significado del texto** para la b√∫squeda. Convierte tanto los documentos como las preguntas del usuario en vectores num√©ricos.

> **‚ö†Ô∏è ¬°ADVERTENCIA CR√çTICA!**
> El modelo de embedding que uses para la **ingesta (`ingest.py`)** y para las **consultas (`app.py`)** debe ser **EXACTAMENTE EL MISMO**. Nuestro sistema tiene una validaci√≥n autom√°tica para prevenir este error, pero es fundamental que tu configuraci√≥n sea consistente. Si cambias el modelo de embedding, **debes borrar la carpeta `/storage` y volver a ejecutar la ingesta.**

### Opciones de Proveedores:

#### a) `openai`
-   **Descripci√≥n**: Modelos de embedding de muy alta calidad y optimizados.
-   **Ejemplos de Modelos (`default_model`)**:
    -   `"text-embedding-3-large"`: El m√°s potente.
    -   `"text-embedding-3-small"`: **(Recomendado)** Gran balance rendimiento/costo.
    -   `"text-embedding-ada-002"`: Modelo de la generaci√≥n anterior, m√°s econ√≥mico.

#### b) `ollama`
-   **Descripci√≥n**: Modelos de embedding que se ejecutan localmente.
-   **Ejemplos de Modelos (`default_model`)**:
    -   `"nomic-embed-text"`: **(Recomendado para local)** Modelo de alto rendimiento dise√±ado espec√≠ficamente para embeddings.
    -   `"all-minilm"`: Un modelo m√°s ligero y r√°pido.

#### c) `huggingface`
-   **Descripci√≥n**: Acceso a miles de modelos de embedding del Hub.
-   **Ejemplos de Modelos (`default_model`)**:
    -   `"BAAI/bge-large-en-v1.5"`
    -   `"sentence-transformers/all-MiniLM-L6-v2"`

---

## üõ†Ô∏è 3. Ejemplos de Archivos `rag_config.json`

#### Escenario 1: M√°ximo Rendimiento (usando OpenAI)
```json
{
  "llm_config": {
    "default_provider": "openai",
    "default_model": "gpt-4o-mini"
  },
  "embedding_config": {
    "default_provider": "openai",
    "default_model": "text-embedding-3-small"
  },
  // ...
}
```

#### Escenario 2: Totalmente Local y Privado (usando Ollama)
```json
{
  "llm_config": {
    "default_provider": "ollama",
    "default_model": "llama3"
  },
  "embedding_config": {
    "default_provider": "ollama",
    "default_model": "nomic-embed-text"
  },
  // ...
}
```

#### Escenario 3: H√≠brido (LLM potente, embeddings locales)
```json
{
  "llm_config": {
    "default_provider": "openai",
    "default_model": "gpt-4o-mini"
  },
  "embedding_config": {
    "default_provider": "ollama",
    "default_model": "nomic-embed-text"
  },
  // ...
}
```
*Recuerda que si usas esta configuraci√≥n, debes ejecutar `python ingest.py --provider ollama --model nomic-embed-text` y luego `python app.py --llm-provider openai`.* 